---
title: "Study Thoughts"
# hidden: true
---

This post is to share some of my thoughts of systematically studying statistics, causal inference, sql, machine learning, data structure and algorithms. So far, I have learned most of the fundamental stuff. The next thing is to enhance my understanding and practice with real-world applications.

I have benefited a lot from my econ background. Though, I recognize that there are significant differences between economics, statistics, and data science. For economics, the priority is to examine whether some change in x will cause a proportionate change in y, holding everything else unchanged. In other words, our goal is to exploit the causal relationship between some x and y. If our model is valid, we can make explanation/inference from the estimated model coefficients - or economic behavioral parameters. How do we ensure that our model is valid? Well, we can't. We assume that the model is based on theories that combined with math, statistics and logics. We then use sample data to test our hypothesis - which is what applied economists do. For any applied fields, data is vitally important. In economics, data not only reflects the consequence of economic behavior, but also the incentive behind the behavior. We can't simply assume the sample data is representative of the population of all kinds. We need to think deep about the underlying incentives, how we should design the market or policy regime for different subpopulation. We also need to consider whether subpopulation uncover some sort of selection bias that could distort the market or policy implication - whether or not being selected into a certain subpopulation has been intervened by the design itself. With the solid understanding in economics, we can make reasonable assumptions about model and data - how to design model, how to deal with missing data, how to aggregate/disaggregate data, how to use proxy of data, etc.

What differentiates data science/machine learning (DSML) from economics is the emphasis of forecast. DSML cares less about each parameter, but rather the projection of future. Do we care about one more room increases the price of Houston house by $$$ dollars? Nah. We just want to know given a set of house features, what will be the saling price of the house? Of course, understanding how the different features contribute to the final prediction is good, but with the more complicated model algorithms, it's harder for features to traverse the journey linearly - (generalized) linear relationship is always easier to understand. Nevertheless, selecting an appropriate set of features has been shown to improve the model performance effectively. We should pay attention to both statistical and economical significance when choosing features. DSML also cares about minimizing generalization error - A good model algorithm should generalize well on new data. It would be useless if it reaches near perfect performance on training data, but poorly on test data. It reflects a combat between the bias-variance of the selected model algorithm. DSML strives to find the sweet spot of the bias-variance tradeoff.

For a special route of data science projects, the goal is to examine feature impact by experimental design and A/B testing. For example, whether the size of action button would affect users' intention to click? Whether the price range would affect users' interest to continue browsing? etc. In this case, the hypothesis testing and statistical inference play a larger role. It depends on how to set up experiments, how to sample data (randomized controls vs network effects), how much data it needs, how long time it takes, etc.

In the following, I will illustrate the abovementioned topics in detail.

To be continued ...